{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import iterparse\n",
    "from lxml import html\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import string\n",
    "import logging\n",
    "import simplejson\n",
    "import spacy\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    filename='../log/w2v_etc.log')\n",
    "\n",
    "log = logging.getLogger('w2v_etc.log')\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Streaming xml parser filters on date and writes to csv \n",
    "\"\"\"\n",
    "\n",
    "SOURCE_PATH = '../datasets/stackoverflow_posts.xml'\n",
    "SECONDARY_PATH = '../datasets/stackoverflow_posts_1501_1601.csv'\n",
    "\n",
    "START_DATE = '2015-01-01'\n",
    "STOP_DATE = '2016-01-01'\n",
    "\n",
    "\n",
    "def _paragraph_generator(elem):\n",
    "    \n",
    "    def _assess_date_get_body():\n",
    "        date = elem.get('CreationDate', '')\n",
    "        body = elem.get('Body', '')\n",
    "        if not date or not body or date < START_DATE or date >= STOP_DATE:\n",
    "            return ''\n",
    "        else:\n",
    "            return body\n",
    "    \n",
    "    body = _assess_date_get_body()\n",
    "    if body:\n",
    "\n",
    "        def _gather_context_tag(tag, prefix):\n",
    "            x = elem.get(tag, '')\n",
    "            if x: return prefix + '_' + x\n",
    "            else: return ''\n",
    "\n",
    "        def _gather_context_tags():\n",
    "            user = _gather_context_tag('OwnerUserId', 'USER')\n",
    "            post = _gather_context_tag('Id', 'POST')\n",
    "            tags = [user, post]\n",
    "            return [t for t in tags if t]\n",
    "\n",
    "        def _format_stackoverflow_tags(tags):\n",
    "            tree = html.fromstring(tags)\n",
    "            return ['TAG_' + t.tag for t in tree.xpath('//*')\n",
    "                    if t.tag and not t.tag in ['html', 'body']]\n",
    "\n",
    "        def _gather_stackoverflow_tags():\n",
    "            tags = elem.get('Tags', '')\n",
    "            if tags: return _format_stackoverflow_tags(tags)\n",
    "            else: return []\n",
    "\n",
    "        def _gather_doc2vec_tags():\n",
    "            return _gather_stackoverflow_tags() + _gather_context_tags()\n",
    "\n",
    "        tags = _gather_doc2vec_tags()\n",
    "\n",
    "        def _prepare_words(paragraph):\n",
    "            # keep words, numbers, punctuation; lemmatize;\n",
    "            # join named-entities; lower case\n",
    "            doc = nlp(paragraph)\n",
    "            ents = doc.ents\n",
    "            doc = [token.lemma_ for token in doc\n",
    "                   if token.is_alpha or token.is_punct or token.like_num]\n",
    "            doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "            return [w.lower() for w in doc]\n",
    "\n",
    "            # p = paragraph\n",
    "            # p = p.translate(p.maketrans(\"\",\"\", string.punctuation))\n",
    "            # p = p.translate(p.maketrans(\"   \",\"   \", '\\n\\t'))\n",
    "            # p = p.lower()\n",
    "            # return [w for w in p.split(' ') if w]\n",
    "        \n",
    "        def _gather_paragraphs():\n",
    "            body_tree = html.fromstring(body)\n",
    "            return [p.text for p in body_tree.xpath('//p') if p.text]\n",
    "        \n",
    "        paragraphs = _gather_paragraphs()\n",
    "        \n",
    "        for p in paragraphs:\n",
    "            words = _prepare_words(p)\n",
    "            yield words, tags\n",
    "\n",
    "            \n",
    "def _write_paragraphs(paragraphs, file):\n",
    "    n = 0\n",
    "    for words, tags in paragraphs:\n",
    "        n += 1\n",
    "        file.write(','.join(words))\n",
    "        file.write(';')\n",
    "        file.write(','.join(tags))\n",
    "        file.write('\\n')\n",
    "    return n\n",
    "\n",
    "\n",
    "def write_secondary():\n",
    "    n_elem = 0\n",
    "    n_para = 0\n",
    "    try:\n",
    "        file = open(SECONDARY_PATH, 'w+')\n",
    "        for event, elem in iterparse(SOURCE_PATH):\n",
    "            n_elem += 1\n",
    "            if (n_elem % 100000 == 0): log.info(str(n_para) + ' ' + str(n_elem))\n",
    "            try:\n",
    "                paragraphs = _paragraph_generator(elem)\n",
    "                n_para += _write_paragraphs(paragraphs, file)\n",
    "            except Exception as e:\n",
    "                log.warning((type(e), e))\n",
    "            finally:\n",
    "                elem.clear()\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "\n",
    "write_secondary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class StackOverflowPostIterator():\n",
    "    def __init__(self): pass\n",
    "    def __iter__(self):\n",
    "        with open(SECONDARY_PATH, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line[:-1]\n",
    "                words, tags = (line.split(';') + [''])[0:2]\n",
    "                words = words.split(',')\n",
    "                tags = tags.split(',')\n",
    "                yield LabeledSentence(words=words, tags=tags)\n",
    "\n",
    "\n",
    "SAVE_PATH = '../saved/18mo_180_20_8_5_a.d2v'\n",
    "        \n",
    "def train_and_save():\n",
    "    model = Doc2Vec(StackOverflowPostIterator(),\n",
    "                    size=180, negative=20, window=8, min_count=5,\n",
    "                    iter=1, workers=5, alpha=0.1, sample=1e-5)\n",
    "    model.save(SAVE_PATH)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_and_save()\n",
    "# model = Doc2Vec.load(SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('scrum', 0.714698076248169), ('kanban', 0.5858825445175171), ('methodologies', 0.5715190768241882), ('collaboration', 0.5650316476821899), ('stakeholders', 0.5586098432540894), ('tdd', 0.5405069589614868), ('velasco', 0.5336797833442688), ('dissertation', 0.5329622030258179), ('coached', 0.5323850512504578), ('fanfiction', 0.5321285724639893)]\n",
      "[('crawlerjs', 0.63333660364151), ('nodes', 0.6314841508865356), ('v01031', 0.6245298385620117), ('xdmpnodeuri', 0.5910543203353882), ('assd', 0.5787916779518127), ('companyupdate', 0.5708209872245789), ('chefstacktraceout', 0.5616493225097656), ('hexfield', 0.5563744306564331), ('cusersxxxxxxxnpmnodemodulessailsbinsailsjs', 0.5531876087188721), ('newnode', 0.5505025386810303)]\n",
      "[('pythons', 0.5597633123397827), ('amazingjust', 0.5398553013801575), ('ironpython', 0.5287105441093445), ('unicurses', 0.5230951905250549), ('qpython', 0.5211294889450073), ('python26', 0.5179910659790039), ('windowsproblem', 0.5071616172790527), ('apl', 0.5047576427459717), ('skulpt', 0.5005300045013428), ('arcpy', 0.5002931356430054)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar('agile'))\n",
    "print(model.most_similar('node'))\n",
    "print(model.most_similar('python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "for sentence in StackOverflowPostIterator():\n",
    "    x = sentence\n",
    "    n -= 1\n",
    "    if (n < 0):\n",
    "        break\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "SOURCE_PATH = '../datasets/stackoverflow_posts.xml'\n",
    "\n",
    "n = 0\n",
    "m = 0\n",
    "keys = set([])\n",
    "min_date = None\n",
    "max_date = None\n",
    "start_time = datetime.now()\n",
    "\n",
    "for event, elem in iterparse(SOURCE_PATH):\n",
    "    n += 1\n",
    "    keys.update(elem.keys())\n",
    "    date = elem.get('CreationDate', '')\n",
    "    body = elem.get('Body', '')\n",
    "    elem.clear()\n",
    "    if min_date is None or (date and date < min_date): min_date = date\n",
    "    if max_date is None or (date and date > max_date): max_date = date\n",
    "    if date and body and date >= '2015-01-01' and date < '2016-01-01': m += 1\n",
    "    if (n % 1000000 == 0): print(m, n, min_date, max_date, keys)\n",
    "\n",
    "end_time = datetime.now()\n",
    "\n",
    "print(n, (end_time - start_time).total_seconds())\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
