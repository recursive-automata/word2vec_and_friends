{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Munge\n",
    "\n",
    "Stack Exchange, Inc. is very generous and does data dumps of the stackoverflows'\n",
    "site posts (find the link below). There's a good spectrum of dataset sizes from\n",
    "their distinct subdomains, and if you're fearless, you can do the whole shabang,\n",
    "probably more than a TB uncompressed.\n",
    "\n",
    "I settled on a few 10s of GB, but that's a bit much to sit comfortably in my box's\n",
    "memory (maybe yours, too), so I setup a streaming xml parser to do the preproccessing\n",
    "and write the results to a few tsv's.\n",
    "\n",
    "Applications to follow...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import iterparse\n",
    "from lxml import html\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import string\n",
    "import logging\n",
    "import simplejson\n",
    "import spacy\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    filename='../log/w2v_etc.log')\n",
    "\n",
    "log = logging.getLogger('w2v_etc.log')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Streaming xml parser filters on date and writes to tsv \n",
    "\"\"\"\n",
    "\n",
    "START_YEAR = 2009\n",
    "STOP_YEAR = 2017\n",
    "\n",
    "SOURCE_PATH = '../datasets/stackoverflow_posts/stackoverflow_posts.xml'\n",
    "SECONDARY_FOLDER = '../datasets/stackoverflow_posts/'\n",
    "\n",
    "years = list(range(START_YEAR, STOP_YEAR))\n",
    "filepaths = [SECONDARY_FOLDER + str(year) + '.tsv' for year in years]\n",
    "\n",
    "\n",
    "def _paragraph_generator(elem):\n",
    "    \n",
    "    def _assess_date_get_body():\n",
    "        date = elem.get('CreationDate', '')\n",
    "        body = elem.get('Body', '')\n",
    "        if not date or not body or date < str(START_YEAR) or date > str(STOP_YEAR):\n",
    "            return '', date\n",
    "        else:\n",
    "            return body, date\n",
    "    \n",
    "    body, date = _assess_date_get_body()\n",
    "    if body:\n",
    "\n",
    "        def _gather_context_tag(tag, prefix):\n",
    "            x = elem.get(tag, '')\n",
    "            if x: return prefix + '_' + x\n",
    "            else: return ''\n",
    "\n",
    "        def _gather_context_tags():\n",
    "            user = _gather_context_tag('OwnerUserId', 'USER')\n",
    "            post = _gather_context_tag('Id', 'POST')\n",
    "            tags = [user, post]\n",
    "            return [t for t in tags if t]\n",
    "\n",
    "        def _format_stackoverflow_tags(tags):\n",
    "            tree = html.fromstring(tags)\n",
    "            return ['TAG_' + t.tag for t in tree.xpath('//*')\n",
    "                    if t.tag and not t.tag in ['html', 'body']]\n",
    "\n",
    "        def _gather_stackoverflow_tags():\n",
    "            tags = elem.get('Tags', '')\n",
    "            if tags: return _format_stackoverflow_tags(tags)\n",
    "            else: return []\n",
    "\n",
    "        def _gather_doc2vec_tags():\n",
    "            return _gather_stackoverflow_tags() + _gather_context_tags()\n",
    "\n",
    "        tags = _gather_doc2vec_tags()\n",
    "        \n",
    "        def _process_token(token):\n",
    "            \"\"\"spaCy Token -> str\"\"\"\n",
    "            if token.lemma_: return token.lemma_.lower()\n",
    "            # elif token.like_num: return 'NUMBER'\n",
    "            else: return token.text.lower()\n",
    "        \n",
    "        def _prepare_words(paragraph):\n",
    "            # keep words, numbers, punctuation; lemmatize;\n",
    "            # join named-entities; lower case\n",
    "            doc = nlp(paragraph)\n",
    "            ents = doc.ents\n",
    "            doc = [_process_token(token) for token in doc\n",
    "                   if token.is_alpha or token.is_punct or token.like_num]\n",
    "            doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "            return doc\n",
    "        \n",
    "        def _gather_paragraphs():\n",
    "            body_tree = html.fromstring(body)\n",
    "            return [p.text for p in body_tree.xpath('//p') if p.text]\n",
    "        \n",
    "        paragraphs = _gather_paragraphs()\n",
    "        \n",
    "        for p in paragraphs:\n",
    "            words = _prepare_words(p)\n",
    "            yield date, words, tags\n",
    "\n",
    "\n",
    "def _which_year_index(date):\n",
    "    date = str(date)\n",
    "    i = -1\n",
    "    for y in years:\n",
    "        i += 1\n",
    "        if str(y) < date < str(y + 1): return i\n",
    "    return None\n",
    "\n",
    "            \n",
    "def _write_paragraphs(paragraphs, files):\n",
    "    n = 0\n",
    "    for date, words, tags in paragraphs:\n",
    "        n += 1\n",
    "        file = files[_which_year_index(date)]\n",
    "        file.write('\\t'.join(words))\n",
    "        file.write('\\t\\t')\n",
    "        file.write('\\t'.join(tags))\n",
    "        file.write('\\n')\n",
    "    return n\n",
    "\n",
    "\n",
    "def write_secondaries():\n",
    "    n_elem = 0\n",
    "    n_para = 0\n",
    "    try:\n",
    "        files = [open(filepath, 'w+') for filepath in filepaths]\n",
    "        for event, elem in iterparse(SOURCE_PATH):\n",
    "            n_elem += 1\n",
    "            if (n_elem % 100000 == 0):\n",
    "                msg = \"parsed {} paragraphs, read {} elements\".format(n_para, n_elem)\n",
    "                log.info(msg)\n",
    "            try:\n",
    "                paragraphs = _paragraph_generator(elem)\n",
    "                n_para += _write_paragraphs(paragraphs, files)\n",
    "            except Exception as e:\n",
    "                log.warning((type(e), e))\n",
    "            finally:\n",
    "                elem.clear()\n",
    "    finally:\n",
    "        [file.close() for file in files]\n",
    "        msg = \"parsed {} paragraphs, read {} elements\".format(n_para, n_elem)\n",
    "        log.info(msg)\n",
    "\n",
    "        \n",
    "write_secondaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATHS = ['../datasets/stackoverflow_posts/2013.tsv']\n",
    "SAVE_PATH = '../saved/2013_180_20_8_5.d2v'\n",
    "\n",
    "class StackOverflowPostIterator():\n",
    "    def __init__(self): pass\n",
    "    def __iter__(self):\n",
    "        for file_path in DATA_PATHS:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line[:-1]\n",
    "                    words, tags = (line.split('\\t\\t') + [''])[0:2]\n",
    "                    words = words.split('\\t')\n",
    "                    tags = tags.split('\\t')\n",
    "                    yield LabeledSentence(words=words, tags=tags)\n",
    "\n",
    "        \n",
    "def train_and_save():\n",
    "    model = Doc2Vec(StackOverflowPostIterator(),\n",
    "                    size=180, negative=20, window=8, min_count=5,\n",
    "                    iter=1, workers=5, alpha=0.1, sample=1e-5)\n",
    "    model.save(SAVE_PATH)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_and_save()\n",
    "# model = Doc2Vec.load(SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('scrum', 0.6519924402236938), ('msf', 0.5370078086853027), ('thesis', 0.5179572105407715), ('senior', 0.5103329420089722), ('evolution', 0.5098087787628174), ('kanban', 0.5042551159858704), ('mature', 0.4935457706451416), ('cto', 0.4915340542793274), ('sprints', 0.48361465334892273), ('wrox', 0.4827408790588379)]\n",
      "\n",
      "[('nodes', 0.7072689533233643), ('leaf', 0.5357713103294373), ('nodeset', 0.5138298273086548), ('newnode', 0.49955424666404724), ('cluster', 0.49572986364364624), ('childnodes', 0.48936402797698975), ('45892', 0.478634774684906), ('subnodes', 0.4744775593280792), ('child', 0.4739358425140381), ('subgraph', 0.47250238060951233)]\n",
      "\n",
      "[('ironpython', 0.5724384784698486), ('lua', 0.5550795793533325), ('distutils', 0.543355405330658), ('java', 0.5419704914093018), ('bash', 0.5343711376190186), ('perl', 0.5284351706504822), ('2.7.8', 0.5262273550033569), ('cpython', 0.5256874561309814), ('3.4', 0.5247158408164978), ('jython', 0.5143867135047913)]\n"
     ]
    }
   ],
   "source": [
    "# word similarities\n",
    "\n",
    "print(model.most_similar('agile'))\n",
    "print('')\n",
    "print(model.most_similar('node'))\n",
    "print('')\n",
    "print(model.most_similar('python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER_843348 ['multinomial', 'multiclass', 'variational', 'conjugate', 'tensor', 'dijkstras', 'dxd', 'posterior', 'lanczos', 'logit']\n",
      "USER_2484687 ['svms', 'tanh', 'softmax', 'regularization', 'posterior', 'svd', 'multiclass', 'bayesian', 'rbf', 'unsupervised']\n",
      "USER_1166362 ['sortwknn', 'maiores', 'calculcate', 'radiance', 'betrounds', 'qaudiooutput', 'reml', 'standarization', 'quickhull', 'piecewise']\n",
      "USER_3942586 ['cepstral', 'kernlab', 'mtx', 'mirza', 'lup', '1607', 'expm', 'munkres', 'diagonalisation', 'pseudoinverse']\n"
     ]
    }
   ],
   "source": [
    "search_type = 'USER'\n",
    "postive_search_vectors = [model['factor'], model['svd'], model['decomposition']]\n",
    "negative_search_vectors = [model['diagonalisation']]\n",
    "\n",
    "found_tags = model.docvecs.most_similar(postive_search_vectors, negative_search_vectors)\n",
    "found_post_tags = [t for t,_ in found_tags if t[0:len(search_type)] == search_type]\n",
    "for t in found_post_tags:\n",
    "    post_vector = model.docvecs[t]\n",
    "    post_words = [w for w,_ in model.most_similar([post_vector])]\n",
    "    print(t, post_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST_21847418 ['syms', 'recreational', 'nrz', 'getlevel', 'unitless', 'alltough', 'lmertest', 'trignometric', 'storagelocation', '1728']\n",
      "POST_26934178 ['fortran', 'thtat', 'compaq', 'apl', 'rutines', 'bdfl', 'gfortran', 'leakoff', '1539', 'generalising']\n",
      "POST_21330945 ['natty', 'qaudiooutput', 'innerchild', 'lanczos', 'artistic', 'exploiting', 'distinctions', 'amenable', 'interoperable', 'relaxed']\n",
      "POST_21886516 ['numerical', 'chemical', 'subscripts', 'reals', 'conditioned', 'superscripts', 'numerically', 'dissipation', 'dbarithmeticexpression', 'numeric']\n",
      "POST_21078051 ['regionerate', 'mpir', 'daqmx', '5.80', 'convertors', '2.99', 'playerobject', 'aussi', 'ligatures', '10.2.0']\n",
      "POST_25493270 ['mpich', 'hpc', 'openib', 'openmpi', '2.6.9', 'mpi', 'galsim', 'withhold', 'mlpack', 'infiniband']\n",
      "POST_25383677 ['fortran', 'apl', '1539', 'befunge', 'culturally', 'dyalog', 'mkl', 'openacc', 'pgi', '1325']\n",
      "POST_27199392 ['multinomial', 'kernlab', 'covariate', 'fitpack', 'borehole', 'mle', 'virginica', 'ashton', 'oscillatory', 'tobit']\n",
      "POST_21230073 ['hickey', 'dearth', 'jaydebeapi', 'whitepapers', 'pylauncher', 'richer', 'authoring', 'framemaker', 'mortals', 'adaptability']\n",
      "POST_23980931 ['committee', 'crockford', '11.3', 'kernighan', 'algol', 'paradigms', 'lrm', 'ritchie', 'vhdl', 'envisioned']\n"
     ]
    }
   ],
   "source": [
    "search_type = 'POST'\n",
    "postive_search_vectors = [model['fortran'], model['numerical']]\n",
    "negative_search_vectors = [model['pointer']]\n",
    "\n",
    "found_tags = model.docvecs.most_similar(postive_search_vectors, negative_search_vectors)\n",
    "found_post_tags = [t for t,_ in found_tags if t[0:len(search_type)] == search_type]\n",
    "for t in found_post_tags:\n",
    "    post_vector = model.docvecs[t]\n",
    "    post_words = [w for w,_ in model.most_similar([post_vector])]\n",
    "    print(t, post_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
