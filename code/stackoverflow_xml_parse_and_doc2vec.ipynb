{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import iterparse\n",
    "from lxml import html\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import string\n",
    "import logging\n",
    "import simplejson\n",
    "import spacy\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    filename='../log/w2v_etc.log')\n",
    "\n",
    "log = logging.getLogger('w2v_etc.log')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Streaming xml parser filters on date and writes to csv \n",
    "\"\"\"\n",
    "\n",
    "START_YEAR = 2009\n",
    "STOP_YEAR = 2017\n",
    "\n",
    "SOURCE_PATH = '../datasets/stackoverflow_posts/stackoverflow_posts.xml'\n",
    "SECONDARY_FOLDER = '../datasets/stackoverflow_posts/'\n",
    "\n",
    "years = list(range(START_YEAR, STOP_YEAR))\n",
    "filepaths = [SECONDARY_FOLDER + str(year) + '.tsv' for year in years]\n",
    "\n",
    "\n",
    "def _paragraph_generator(elem):\n",
    "    \n",
    "    def _assess_date_get_body():\n",
    "        date = elem.get('CreationDate', '')\n",
    "        body = elem.get('Body', '')\n",
    "        if not date or not body or date < str(START_YEAR) or date > str(STOP_YEAR):\n",
    "            return '', date\n",
    "        else:\n",
    "            return body, date\n",
    "    \n",
    "    body, date = _assess_date_get_body()\n",
    "    if body:\n",
    "\n",
    "        def _gather_context_tag(tag, prefix):\n",
    "            x = elem.get(tag, '')\n",
    "            if x: return prefix + '_' + x\n",
    "            else: return ''\n",
    "\n",
    "        def _gather_context_tags():\n",
    "            user = _gather_context_tag('OwnerUserId', 'USER')\n",
    "            post = _gather_context_tag('Id', 'POST')\n",
    "            tags = [user, post]\n",
    "            return [t for t in tags if t]\n",
    "\n",
    "        def _format_stackoverflow_tags(tags):\n",
    "            tree = html.fromstring(tags)\n",
    "            return ['TAG_' + t.tag for t in tree.xpath('//*')\n",
    "                    if t.tag and not t.tag in ['html', 'body']]\n",
    "\n",
    "        def _gather_stackoverflow_tags():\n",
    "            tags = elem.get('Tags', '')\n",
    "            if tags: return _format_stackoverflow_tags(tags)\n",
    "            else: return []\n",
    "\n",
    "        def _gather_doc2vec_tags():\n",
    "            return _gather_stackoverflow_tags() + _gather_context_tags()\n",
    "\n",
    "        tags = _gather_doc2vec_tags()\n",
    "        \n",
    "        def _process_token(token):\n",
    "            \"\"\"spaCy Token -> str\"\"\"\n",
    "            if token.lemma_: return token.lemma_.lower()\n",
    "            # elif token.like_num: return 'NUMBER'\n",
    "            else: return token.text.lower()\n",
    "        \n",
    "        def _prepare_words(paragraph):\n",
    "            # keep words, numbers, punctuation; lemmatize;\n",
    "            # join named-entities; lower case\n",
    "            doc = nlp(paragraph)\n",
    "            ents = doc.ents\n",
    "            doc = [_process_token(token) for token in doc\n",
    "                   if token.is_alpha or token.is_punct or token.like_num]\n",
    "            doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "            return doc\n",
    "        \n",
    "        def _gather_paragraphs():\n",
    "            body_tree = html.fromstring(body)\n",
    "            return [p.text for p in body_tree.xpath('//p') if p.text]\n",
    "        \n",
    "        paragraphs = _gather_paragraphs()\n",
    "        \n",
    "        for p in paragraphs:\n",
    "            words = _prepare_words(p)\n",
    "            yield date, words, tags\n",
    "\n",
    "\n",
    "def _which_year_index(date):\n",
    "    date = str(date)\n",
    "    i = -1\n",
    "    for y in years:\n",
    "        i += 1\n",
    "        if str(y) < date < str(y + 1): return i\n",
    "    return None\n",
    "\n",
    "            \n",
    "def _write_paragraphs(paragraphs, files):\n",
    "    n = 0\n",
    "    for date, words, tags in paragraphs:\n",
    "        n += 1\n",
    "        file = files[_which_year_index(date)]\n",
    "        file.write('\\t'.join(words))\n",
    "        file.write('\\t\\t')\n",
    "        file.write(','.join(tags))\n",
    "        file.write('\\n')\n",
    "    return n\n",
    "\n",
    "\n",
    "def write_secondaries():\n",
    "    n_elem = 0\n",
    "    n_para = 0\n",
    "    try:\n",
    "        files = [open(filepath, 'w+') for filepath in filepaths]\n",
    "        for event, elem in iterparse(SOURCE_PATH):\n",
    "            n_elem += 1\n",
    "            if (n_elem % 100000 == 0):\n",
    "                msg = \"parsed {} paragraphs, read {} elements\".format(n_para, n_elem)\n",
    "                log.info(msg)\n",
    "            try:\n",
    "                paragraphs = _paragraph_generator(elem)\n",
    "                n_para += _write_paragraphs(paragraphs, files)\n",
    "            except Exception as e:\n",
    "                log.warning((type(e), e))\n",
    "            finally:\n",
    "                elem.clear()\n",
    "    finally:\n",
    "        [file.close() for file in files]\n",
    "        msg = \"parsed {} paragraphs, read {} elements\".format(n_para, n_elem)\n",
    "        log.info(msg)\n",
    "\n",
    "        \n",
    "write_secondaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATHS = ['../datasets/stackoverflow_posts/2015.tsv']\n",
    "SAVE_PATH = '../saved/2015_180_20_8_5.d2v'\n",
    "\n",
    "class StackOverflowPostIterator():\n",
    "    def __init__(self): pass\n",
    "    def __iter__(self):\n",
    "        for file_path in DATA_PATHS:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line[:-1]\n",
    "                    words, tags = (line.split('\\t\\t') + [''])[0:2]\n",
    "                    words = words.split('\\t')\n",
    "                    tags = tags.split('\\t')\n",
    "                    yield LabeledSentence(words=words, tags=tags)\n",
    "\n",
    "        \n",
    "def train_and_save():\n",
    "    model = Doc2Vec(StackOverflowPostIterator(),\n",
    "                    size=180, negative=20, window=8, min_count=5,\n",
    "                    iter=1, workers=5, alpha=0.1, sample=1e-5)\n",
    "    model.save(SAVE_PATH)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_and_save()\n",
    "# model = Doc2Vec.load(SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# word similarities\n",
    "\n",
    "print(model.most_similar('agile'))\n",
    "print('')\n",
    "print(model.most_similar('node'))\n",
    "print('')\n",
    "print(model.most_similar('python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# searching for java-free posts about linear regression\n",
    "\n",
    "search_type = 'POST'\n",
    "postive_search_vectors = [model['linear'], model['regression']]\n",
    "negative_search_vectors = [model['java']]\n",
    "\n",
    "found_tags = model.docvecs.most_similar(postive_search_vectors, negative_search_vectors)\n",
    "found_post_tags = [t for t,_ in found_tags if t[0:len(search_type)] == search_type]\n",
    "for t in found_post_tags:\n",
    "    post_vector = model.docvecs[t]\n",
    "    post_words = [w for w,_ in model.most_similar([post_vector])]\n",
    "    print(p, post_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# searching for users who post about numerical fortran and not javascript\n",
    "\n",
    "search_type = 'USER'\n",
    "postive_search_vectors = [model['fortran'], model['numerical']]\n",
    "negative_search_vectors = [model['javascript']]\n",
    "\n",
    "found_tags = model.docvecs.most_similar(postive_search_vectors, negative_search_vectors)\n",
    "found_post_tags = [t for t,_ in found_tags if t[0:len(search_type)] == search_type]\n",
    "for t in found_post_tags:\n",
    "    post_vector = model.docvecs[t]\n",
    "    post_words = [w for w,_ in model.most_similar([post_vector])]\n",
    "    print(p, post_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
